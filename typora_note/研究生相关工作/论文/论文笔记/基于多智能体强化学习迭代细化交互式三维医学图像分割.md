# 论文阅读|基于多智能体强化学习迭代细化交互式三维医学图像分割

《Iteratively-Refined Interactive 3D Medical Image Segmentation with Multi-Agent Reinforcement Learning》

基于多智能体强化学习迭代细化交互式三维医学图像分割

https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1911.10334



主体思路是：**每一个体素是一个agent**。怎么理解这句话呢？按照我浅薄的理解：分割其实是对每一个像素做分类，对于3D图来说，应该是对于每一个体素做分类。那么分类的准不准确呢？作者没有**从“边界”的思路**进行调整，也就是作者没有把前景物体的边界作为一个整体去划分，而是一个体素一个体素的判断是不是分割正确。**对于分类不正确的体素点，给它的分割概率加上一个值a，而这个a的值就是网络输出的action**。用这种逐点调整概率的方法来调整粗分割效果。



## 背景

1.现有的自动三维图像分割方法往往不能满足临床应用的需要。

随着卷积神经网络（CNNs）的发展，自动分割大大提高了医学图像处理的效率[13,16,23]。然而，目前的自动检测方法的准确性和鲁棒性仍有待提高，以满足临床实际应用的需要。

2.许多研究都提出了一种交互式策略，通过迭代地结合用户提示来提高图像的分割性能。

这种交互方法通过在预测模型中添加新的标注约束来提高分割的性能，已成为一个热门的研究方向。通常，一次性的交互可能不能确保分割的准确性。因此，现有的许多方法都是兼容迭代细化模式的:**算子根据当前的结果提供新的提示来细化分割，直到达到满意的分割。此外，为了减少交互次数，现有的研究引入了用自动获得的粗分割替换初始提示的思想**

Many studies have ex-plored an interactive strategy to improve the image segmen-
tation performance by iteratively incorporating user hints.
However, the dynamic process for successive interactions
is largely ignored.





目前的更新方法主要存在两个问题:

1)**经常忽略连续交互的动态过程**。虽然分割可以迭代地细化，但模型总是在没有之前信息的情况下，孤立地对待每个细化步骤的分割。

2)另一个问题是，**使用二值分割结果**而不是每个体素的分割概率作为模型输入的一部分时，失去了预测的不确定性。从稠密分割概率到二值分割预测的舍入会造成量化误差和精度损失。



## 方法

本文提出了一种新的交互式医学图像分割更新方法，即**基于多智能体强化学习的迭代微调交互式三维医学图像分割**（IteR-MRL）。
----------------------------------------------------------------------------------------------------------------------------------

**我们将迭代交互式图像分割的动态过程表示为MDP。**（解决了第一个问题）具体地说，在每一个细化步骤中，模型都需要根据先前的分割和交互中的监督信息来决定所有体素的标签。然后，模型将根据预先定义的分割度量得到反馈，并重复上述过程，直到达到最大交互次数。然后我们采用RL方法来求解上述MDP，即寻找在每个细化步骤中获得最大累计反馈的细分策略。

我们提出通过**分割概率来保留预测的不确定性**，这样可以丰富以往分段的信息，使调整更加精确和精细。

--------------------------------------------------------------------------------------------------------------------------------------

现有的基于监督学习的算法的主要问题是它们将整个图像细化过程分**割为孤立的步骤**。为了解决这个问题，我们采用RL，通过将奖励设计为相对改进来明确地捕捉连续预测之间的关系。

由于体素预测的大状态空间和大动作空间以及相互依赖的体素之间协作的必要性，我们采用了MARL的思想:将三维图像中的每一个体素视为一个代理。



*1.利用原始的三维图像、之前的分割概率和交互信息作为state*

2.*中间的actor网络对分割概率进行更新，生成新的分割概率。*（注意，先前的分割概率来自于之前的更新迭代，交互信息是从用户提示转换而来的提示图）

3.actor网络输出agent的动作，调整先前的分割概率，生成当前的分割概率。



4.然后，对当前分割概率进行两个后续操作。

4.1一方面，通过计算基于ground truth和连续分割概率的先前和当前交叉熵的相对增益，向网络反馈一个奖励信号，用于参数更新。

4.2另一方面，它呈现给用户，用户提供反馈，即点击对象或背景，对错误的预测区域。在图2中，单击被表示为一个红色的点，为了便于显示，这个红色点被放大了。

一般以初始方法(任何一种分割方法)产生的粗糙分割概率作为初始分割，IteR-MRL迭代地细化分割概率，直到分割满意为止。



#### 1.1. Multiagent RL framework for interactive image segmentation

x = (x1, ・ ・ ・ , xN)。X是dataset中的任意一张图，xi是x的一个体素。

我们将每个xi作为一个agent，由πi(a(t)i |s(t)i )确定策略。

si(t)分别是状态(图像，之前的分割概率，用户交互) (image, previous
segmentation probability, user interaction)

ai(t) 动作(对先前概率的调整)。(adjustment to previous probability)

for xi at the step t;

By using convolutional kernels, one agent can access
to its neighbors’ states as well, where neighbors are
considered as near voxels.

从整个图像的角度出发，将之前的分割改进为新的分割。通过使用全局action a，图像agent转化为全局state，并得到全局reward。



#### State

体素agent xi 在step t的state是，它本身的体素值bi、它之前的分割概率pi作为目标label，以及它在提示map上的两个值h+和h-：

Si = [bi,pi,h+,h-],p(t)i ∈ [0, 1]

**Hitting map**

第t步的用户交互，提示图h(t)是从用户的点击点形式的提示转换而来的。通过单击给出一个提示点，用户表示它周围的区域是一个错误区域。直观上，一个点离提示点越近，它的标签被错误预测的可能性就越大。 因此提示map表明了提示和局部交互到整个图片的传播的放射区域。

(然后说一些这个的具体)



#### Action. 

对于xi在step t的action ai是用来通过一个**固定的ai**来调整先前的分割概率pi。因此**采取action ai之后**的之后的分割概率pi是：

![img](https://pic1.zhimg.com/80/v2-da2c3900b7eb1c24e3560a77f923a004_1440w.jpg)

Cab（x）是一个把x的值裁剪到a到b的函数。Pi（t+1）是约束到[0,1],因为它代表一个概率。

Action集合A包含了K个action，允许agent在不同的情况下不同程度地调整概率。例如，当一个体素接近提示点击点时，对它进行较大的调整是合理的。此外，当一个体素的大多数相邻体素选择这个动作时，它会倾向于采取这个特定的调整动作。

这边对Action的理解可以理解为戚老哥之前说的 点亮 操作，如果这个像素概率倾向于1的时候，能获得更多的reward的话，那就会更好得倾向于1



**Reward** 

为了提高探索效率，我们设计了一个相对交叉熵增益的奖励机制，以约束的方向更新模型。具体来说，奖励被设计为先前分割到当前分割的相对改进，即ground truth yi与分割概率pi之间的交叉熵Xi的减少。

对于(4)，agent在其概率接近真实体素标签的情况下获得正奖励，反之亦然。相对收益不是一个遥远的目标，而是为代理提供了一个基准来进行比较和超越。

一般来说，一个交互序列的累积回报为

![image-20201030145214560](C:\Users\William_XH\AppData\Roaming\Typora\typora-user-images\image-20201030145214560.png)

T是所有的step数目，折扣因子gamma的值位于(0, 1].



**Network and training**

为了公平比较，我们的算法和其他基线方法都采用了[20]的**交互式网络架构R-net作为主干。**

我们采用图3中的网络来适应RL训练算法:**异步优势actor-critic (A3C)[14]。**

![img](https://pic1.zhimg.com/80/v2-c117cdacb7865253226517e5d9034e94_1440w.jpg)

可以看到

将网络分为两个head:策略head和价值head。

两个头部都有三个三维卷积块，用于提取特定的高级特征。

**策略头**的功能是预测在已知state下的action概率分布。在我们的例子中，给定图像、提示映射和以前的分割概率，policy head预测对以前的分割概率进行每一种尺度的调整的可能性有多大。（一般来说，action是概率最大的那个输出的值）。

Value 头的功能是估计当前state的价值。具体来说，value head评估当前image、提示map和之前分割概率的组合的好坏。







